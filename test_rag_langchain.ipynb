{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Jupyter Notebook Setup for RAG and LangChain Chatbot\n",
    "\n",
    "# ## 1. Setup Environment\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "INDEX_NAME = os.getenv(\"INDEX_NAME\")\n",
    "\n",
    "print(\"Environment variables loaded successfully.\")\n",
    "\n",
    "# ## 2. Install Required Libraries\n",
    "# Make sure Pinecone and other dependencies are installed\n",
    "# Already done via pip command in terminal\n",
    "\n",
    "# ## 3. Data Ingestion Script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tyto4\\OneDrive\\Desktop\\langchainrag_medium\\langchainrag\\.venv\\lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tyto4\\OneDrive\\Desktop\\langchainrag_medium\\langchainrag\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ingestion completed and embeddings stored in Pinecone.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone as LangchainPinecone\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# Initialize the Pinecone client\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Connect to the index\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "# Define the PDF file path\n",
    "pdf_file_path = \"data/Richtlinien Praktikum IWI-AS.pdf\"  # Update this with the correct path to your PDF file\n",
    "\n",
    "# Load PDF content\n",
    "loader = PyPDFLoader(pdf_file_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the document into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(texts)} chunks\")\n",
    "\n",
    "# Create embeddings using OpenAI\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Store the embeddings in Pinecone using Langchain's Pinecone wrapper\n",
    "vectorstore = LangchainPinecone.from_documents(texts, embeddings, index_name=INDEX_NAME)\n",
    "\n",
    "print(\"Data ingestion completed and embeddings stored in Pinecone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tyto4\\OneDrive\\Desktop\\langchainrag_medium\\langchainrag\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1: {'query': 'What are the applications of generative AI according to the paper? Please number each application.', 'result': \"I don't have information on generative AI applications from the provided text.\"}\n",
      "Response 2: {'query': 'Can you please elaborate more on application number 2?', 'result': \"I'm sorry, but there is no specific application number mentioned in the provided text. If you can provide more context or details, I would be happy to help further.\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ## 4. Build a Stateless Chatbot\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Initialize the OpenAI chat model\n",
    "chat = ChatOpenAI(verbose=True, temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Create a RetrievalQA chain\n",
    "qa = RetrievalQA.from_chain_type(llm=chat, chain_type=\"stuff\", retriever=vectorstore.as_retriever())\n",
    "\n",
    "# Ask a question\n",
    "response = qa.invoke(\"What are the applications of generative AI according to the paper? Please number each application.\")\n",
    "print(\"Response 1:\", response)\n",
    "\n",
    "response = qa.invoke(\"Can you please elaborate more on application number 2?\")\n",
    "print(\"Response 2:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tyto4\\OneDrive\\Desktop\\langchainrag_medium\\langchainrag\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1: I don't have information on the applications of generative AI from the provided text.\n",
      "Response 2: I don't have that information.\n"
     ]
    }
   ],
   "source": [
    "# ## 5. Build a Stateful Chatbot\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Initialize chat history\n",
    "chat_history = []\n",
    "\n",
    "# Create a ConversationalRetrievalChain\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=chat, \n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# Ask a question and store the chat history\n",
    "res = qa({\"question\": \"What are the applications of generative AI according to the paper? Please number each application.\", \"chat_history\": chat_history})\n",
    "print(\"Response 1:\", res[\"answer\"])\n",
    "\n",
    "# Store the question-answer pair in chat history\n",
    "history = (res[\"question\"], res[\"answer\"])\n",
    "chat_history.append(history)\n",
    "\n",
    "# Follow-up question\n",
    "res = qa({\"question\": \"Can you please elaborate more on application number 2?\", \"chat_history\": chat_history})\n",
    "print(\"Response 2:\", res[\"answer\"])\n",
    "\n",
    "# Store the follow-up question and answer in chat history\n",
    "history = (res[\"question\"], res[\"answer\"])\n",
    "chat_history.append(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an interactive function to ask questions\n",
    "def ask_question(qa_chain, chat_history):\n",
    "    while True:\n",
    "        # Get user input\n",
    "        question = input(\"You: \")\n",
    "        if question.lower() in ['exit', 'quit']:\n",
    "            print(\"Ending conversation.\")\n",
    "            break\n",
    "\n",
    "        # Pass the question to the ConversationalRetrievalChain\n",
    "        response = qa_chain({\"question\": question, \"chat_history\": chat_history})\n",
    "        \n",
    "        # Print the chatbot's response\n",
    "        print(\"Chatbot:\", response[\"answer\"])\n",
    "        \n",
    "        # Store the question-answer pair in chat history\n",
    "        chat_history.append((question, response[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize chat history\n",
    "chat_history = []\n",
    "\n",
    "# Create the ConversationalRetrievalChain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=chat, \n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# Start the interactive session\n",
    "ask_question(qa_chain, chat_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
